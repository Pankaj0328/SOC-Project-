# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yqUznij0SWTMqwrPAvK9KqHYCsJ3Qoyq
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# Input Training Text
text = input("Enter your training data: ")
chars = sorted(list(set(text)))
vocab_size = len(chars)
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}
def encode(s): return [stoi[c] for c in s]
def decode(indices): return ''.join([itos[i] for i in indices])
data = torch.tensor(encode(text), dtype=torch.long)

# Dynamically adjust block_size
block_size = min(64, max(8, len(data) - 2))
batch_size = 2
embedding_dim = 256
n_heads = 4
n_layers = 5
dropout = 0.1
num_iters = 10000

def get_batch(data, batch_size, block_size):
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

class SelfAttention(nn.Module):
    def __init__(self, emb_dim, head_size):
        super().__init__()
        self.key = nn.Linear(emb_dim, head_size, bias=False)
        self.query = nn.Linear(emb_dim, head_size, bias=False)
        self.value = nn.Linear(emb_dim, head_size, bias=False)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)
        attn_scores = q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5)
        mask = torch.tril(torch.ones(T, T, device=x.device))
        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        out = attn_weights @ v
        return out

class TransformerBlock(nn.Module):
    def __init__(self, emb_dim, n_heads):
        super().__init__()
        head_size = emb_dim // n_heads
        self.heads = nn.ModuleList([SelfAttention(emb_dim, head_size) for _ in range(n_heads)])
        self.proj = nn.Linear(emb_dim, emb_dim)
        self.dropout = nn.Dropout(dropout)
        self.ln1 = nn.LayerNorm(emb_dim)
        self.ln2 = nn.LayerNorm(emb_dim)
        self.ff = nn.Sequential(
            nn.Linear(emb_dim, 4 * emb_dim),
            nn.ReLU(),
            nn.Linear(4 * emb_dim, emb_dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        x = self.ln1(x)
        attn = torch.cat([h(x) for h in self.heads], dim=-1)
        x = x + self.dropout(self.proj(attn))
        x = x + self.ff(self.ln2(x))
        return x

class MiniGPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, embedding_dim)
        self.pos_emb = nn.Embedding(block_size, embedding_dim)
        self.blocks = nn.Sequential(*[TransformerBlock(embedding_dim, n_heads) for _ in range(n_layers)])
        self.fc = nn.Linear(embedding_dim, vocab_size)
    def forward(self, x):
        B, T = x.shape
        token = self.token_emb(x)
        position = self.pos_emb(torch.arange(T, device=x.device)).unsqueeze(0)
        x = token + position
        x = self.blocks(x)
        logits = self.fc(x)
        return logits
    @torch.no_grad()
    def generate(self, idx, max_new):
        self.eval()
        for _ in range(max_new):
            idx_cond = idx[:, -block_size:]
            logits = self(idx_cond)
            logits = logits[:, -1, :]
            probs = F.softmax(logits, dim=-1)
            next = probs.argmax(dim=-1, keepdim=True)
            idx = torch.cat([idx, next], dim=1)
        return idx

torch.manual_seed(42)
model = MiniGPT()
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)

for iter in range(num_iters):
    x_batch, y_batch = get_batch(data, batch_size, block_size)
    logits = model(x_batch)
    loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if iter % 1000 == 0 or iter == num_iters - 1:
        print(f"Iter {iter} | Loss {loss.item():.4f}")

prompt = text[:min(60, len(text))]
start = torch.tensor([[stoi[c] for c in prompt]], dtype=torch.long)
out = model.generate(start, max_new=(len(text) - len(prompt)))[0].tolist()
print("\nGenerated Text:\n", decode(out))